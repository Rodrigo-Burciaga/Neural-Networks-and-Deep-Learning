{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of NN from scratch 1","version":"0.3.2","provenance":[{"file_id":"1oS7NtRpa0IC_d0fNDsf_DNO7HfmUcVQm","timestamp":1564793862009}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9PwxdHcvp1NA","colab_type":"code","colab":{}},"source":["#importing dependencies\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import gzip\n","import numpy as np\n","import random\n","import copy\n","try:\n","    import cPickle as pickle\n","except:\n","    import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNyunLupc-sS","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"vMdbpi52XYcd","colab_type":"code","outputId":"3c6cf88c-37a1-46a8-c998-43e196169b7e","executionInfo":{"status":"ok","timestamp":1564801296676,"user_tz":300,"elapsed":3234,"user":{"displayName":"Rodrigo Andrés Burciaga Ornelas","photoUrl":"","userId":"01811326954830511276"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["!git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'neural-networks-and-deep-learning'...\n","remote: Enumerating objects: 1163, done.\u001b[K\n","remote: Total 1163 (delta 0), reused 0 (delta 0), pack-reused 1163\u001b[K\n","Receiving objects: 100% (1163/1163), 20.42 MiB | 36.95 MiB/s, done.\n","Resolving deltas: 100% (577/577), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0mPqTzwDbqfO","colab_type":"code","outputId":"ef09d511-dd21-44f1-e7bf-aa94288eff68","executionInfo":{"status":"ok","timestamp":1564801296830,"user_tz":300,"elapsed":417,"user":{"displayName":"Rodrigo Andrés Burciaga Ornelas","photoUrl":"","userId":"01811326954830511276"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd neural-networks-and-deep-learning/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/neural-networks-and-deep-learning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mfFSaYAMYWxW","colab_type":"code","colab":{}},"source":["\"\"\"Class whose aim is to load data from the serialization file, and give it a\n","format to can be used in our code\"\"\"\n","class DataLoader(object):\n","  \n","  def load_data(self):\n","    with gzip.open('data/mnist.pkl.gz','rb') as ff :\n","        u = pickle._Unpickler( ff )\n","        u.encoding = 'latin1'\n","        training_data, validation_data, test_data= u.load()\n","    return (training_data, validation_data, test_data)\n","  \n","  def load_data_wrapper(self):\n","    tr_d, va_d, te_d = self.load_data()\n","    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n","    training_results = [ self.vectorized_result(y) for y in tr_d[1]]\n","    training_data = zip(training_inputs, training_results)\n","    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n","    validation_data = zip(validation_inputs, va_d[1])\n","    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n","    test_data = zip(test_inputs, te_d[1])\n","    return (training_data, validation_data, test_data)\n","  \n","  def vectorized_result(self, j):\n","    e = np.zeros((10, 1))\n","    e[j] = 1.0\n","    return e"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mo5YWQ0qiVeU","colab_type":"code","colab":{}},"source":["data_loader = DataLoader()\n","training_data, validation_data, test_data = data_loader.load_data_wrapper()\n","training_data, validation_data, test_data = list(training_data), \\\n","list(validation_data), list(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jeYfgbimLMa","colab_type":"code","outputId":"41dace75-01e0-499c-e72a-d862b1872bb8","executionInfo":{"status":"ok","timestamp":1564794850636,"user_tz":300,"elapsed":413,"user":{"displayName":"Rodrigo Andrés Burciaga Ornelas","photoUrl":"","userId":"01811326954830511276"}},"colab":{"base_uri":"https://localhost:8080/","height":298}},"source":["#inspecting the data\n","print(training_data[0][0].shape)\n","img = np.reshape(training_data[0][0], (28,28))\n","plt.imshow(img, cmap='gray')\n","title_obj = plt.title('Número: {}'.format(np.argmax(training_data[0][1])))\n","plt.setp(title_obj, color='w')  \n","plt.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(784, 1)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv0hQCr9IUAq/SFD/D4ICDdG2PK+CAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"KaQ3AY9sIC5b","colab_type":"text"},"source":["#  Neural Network "]},{"cell_type":"code","metadata":{"id":"jjusEVNtI2xj","colab_type":"code","colab":{}},"source":["\"\"\"A class that implement a very basic SGD with sigmoid function, Gradients are \n","calculated using backpropagation\"\"\"\n","\n","class NeuralNetwork(object):\n","  \n","  @staticmethod\n","  def sigmoid(z): \n","    return 1.0/(1.0+np.exp(-z))\n","  \n","  @staticmethod\n","  def sigmoid_prime(z):\n","    \"\"\"\"Derivative of the sigmoid function.\"\"\"\n","    return NeuralNetwork.sigmoid(z) * (1 - NeuralNetwork.sigmoid(z))\n","  \n","  def __init__(self, sizes):\n","    self.num_layers = len(sizes)\n","    self.sizes = sizes \n","    self.biases = [ np.random.rand(y,1) for y in sizes[1:]]\n","    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n","    \n","  def print_shapes(self):\n","    for weights, bias  in zip(self.weights, self.biases):\n","      print(weights.shape, bias.shape)\n","      \n","  def feedforward(self, a):\n","    for b, w in zip(self.biases, self.weights):\n","      a = NeuralNetwork.sigmoid(np.dot(w,a) + b)\n","    return a\n","  \n","  \"\"\"Method that develops Stochastic Gradient Descent using mini batch\"\"\"\n","  def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","    if test_data:\n","      n_test = len(test_data)\n","    n = len(training_data)\n","    for j in range(epochs):\n","      random.shuffle(training_data)\n","      mini_batches = [\n","                training_data[k:k+mini_batch_size]\n","           for k in range(0, n, mini_batch_size)]\n","      for mini_batch in mini_batches:\n","        self.update_mini_batch(mini_batch, eta)\n","      if test_data:\n","        print(\"Epoch {0}: {1} / {2}\".format( j, \n","                                            self.evaluate(test_data), n_test))\n","      else:\n","        print(\"Epoch {0} complete\".format(j))\n","  \n","  def update_mini_batch(self, mini_batch, eta):\n","    \"\"\"Update the network’s weights and biases by applying\n","    gradient descent using backpropagation to a single mini batch.\n","    The ‘‘mini_batch ‘‘ is a list of tuples ‘‘(x, y)‘‘, and ‘‘eta‘‘\n","    is the learning rate.\"\"\"\n","    x, y = self.flat_batch(mini_batch)\n","    db, dw = self.backprop2(x,y)\n","    #bi = copy.deepcopy(self.biases)\n","    #wi = copy.deepcopy(self.weights)\n","    #wi = [w- (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, dw)]\n","    #bi = [b- (eta/len(mini_batch)) * nb for b, nb in zip(self.biases, db)]\n","    self.weights = [w- (eta/len(mini_batch)) * nw \n","                    for w, nw in zip(self.weights, dw)]\n","    self.biases = [b- (eta/len(mini_batch)) * nb \n","                   for b, nb in zip(self.biases, db)]\n","    \n","    '''for w in wi:\n","      print(np.sum(w), w.shape)\n","    \n","    nabla_b = [np.zeros(b.shape) for b in self.biases]\n","    nabla_w = [np.zeros(w.shape) for w in self.weights]\n","    for x, y in mini_batch:\n","      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n","      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n","      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n","    self.weights = [w-(eta/len(mini_batch)) * nw\n","                        for w, nw in zip(self.weights, nabla_w)]\n","    self.biases = [b-(eta/len(mini_batch)) * nb \n","                   for b, nb in zip(self.biases, nabla_b)]\n","    for w in self.weights:\n","      print(np.sum(w), w.shape)\n","      '''\n","    \n","  def flat_batch(self, mini_batch):\n","    size = len(mini_batch)\n","    unziped = list(zip(*mini_batch)) \n","    x = np.asarray(unziped[0])\n","    x = x.reshape(size, x.shape[1])\n","    y = np.asarray(unziped[1])\n","    y = y.reshape(size, y.shape[1])\n","    return x.T, y.T\n","      \n","  def backprop(self, x, y):\n","    nabla_b = [np.zeros(b.shape) for b in self.biases]\n","    nabla_w = [np.zeros(w.shape) for w in self.weights]\n","    # feedforward\n","    activation = x\n","    activations = [x] # list to store all the activations, layer by layer\n","    zs = [] # list to store all the z vectors, layer by layer\n","    for b, w in zip(self.biases, self.weights):\n","      z = np.dot(w, activation) + b\n","      zs.append(z)\n","      #we are using sigmoid function for all layers\n","      activation = NeuralNetwork.sigmoid(z) \n","      activations.append(activation)\n","    delta = self.cost_derivative(activations[-1], y) * \\\n","    NeuralNetwork.sigmoid_prime(zs[-1])\n","    nabla_b[-1] = delta\n","    nabla_w[-1] = np.dot(delta, activations[-2].T)\n","    for l in range (2,self.num_layers):\n","      z = zs[-l]\n","      sp = NeuralNetwork.sigmoid_prime(z)\n","      delta = np.dot(self.weights[-l+1].T, delta) * sp\n","      nabla_b[-l] = delta\n","      nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n","      return (nabla_b, nabla_w)\n","  \n","  \n","  def backprop2(self, x, y):\n","    nabla_b = [np.zeros(b.shape) for b in self.biases]\n","    nabla_w = [np.zeros(w.shape) for w in self.weights]\n","    # feedforward\n","    activation = x\n","    activations = [x] # list to store all the activations, layer by layer\n","    zs = [] # list to store all the z vectors, layer by layer\n","    for b, w in zip(self.biases, self.weights):\n","      z = np.dot(w, activation) + b\n","      zs.append(z)\n","      #we are using sigmoid function for all layers\n","      activation = NeuralNetwork.sigmoid(z) \n","      activations.append(activation)\n","    delta = self.cost_derivative(activations[-1], y) * \\\n","    NeuralNetwork.sigmoid_prime(zs[-1])\n","    nabla_b[-1] = np.sum(delta, axis = 1).reshape(delta.shape[0],1)\n","    nabla_w[-1] = np.dot(delta, activations[-2].T) ## 10,100 100,15\n","    #nabla_w[-1] = np.dot(delt, act.T)\n","    \n","    for l in range (2,self.num_layers):\n","      z = zs[-l]\n","      sp = NeuralNetwork.sigmoid_prime(z)\n","      delta = np.dot(self.weights[-l+1].T, delta) * sp\n","      nabla_b[-l] = np.sum(delta, axis = 1).reshape(delta.shape[0],1)\n","      nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n","      return (nabla_b, nabla_w)\n","\n","  def evaluate(self, test_data):\n","    \"\"\"Return the number of test inputs for which the neural\n","    network outputs the correct result. Note that the neural\n","    network's output is assumed to be the index of whichever\n","    neuron in the final layer has the highest activation.\"\"\"\n","    test_results = [(np.argmax(self.feedforward(x)), y)\n","                    for (x, y) in test_data]\n","    return sum(int(x == y) for (x, y) in test_results)\n","    \n","  def cost_derivative(self, output_activations, y):\n","    \"\"\"The derivative of cuadratic error = a-y where a is the final output \n","    activation and y is the ground truth label\"\"\"\n","    return (output_activations - y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCOmCr8QfCLB","colab_type":"code","outputId":"ff21edb2-8406-4b6e-9ed0-6f99288683ff","executionInfo":{"status":"ok","timestamp":1564803845026,"user_tz":300,"elapsed":343,"user":{"displayName":"Rodrigo Andrés Burciaga Ornelas","photoUrl":"","userId":"01811326954830511276"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["nn = NeuralNetwork([784,100,10])\n","nn.print_shapes()"],"execution_count":50,"outputs":[{"output_type":"stream","text":["(100, 784) (100, 1)\n","(10, 100) (10, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TeU3AydQ5XLl","colab_type":"code","outputId":"9f78a0fe-203a-45de-adfc-33487a4a5ac9","executionInfo":{"status":"ok","timestamp":1564804011189,"user_tz":300,"elapsed":164880,"user":{"displayName":"Rodrigo Andrés Burciaga Ornelas","photoUrl":"","userId":"01811326954830511276"}},"colab":{"base_uri":"https://localhost:8080/","height":538}},"source":["nn.SGD(training_data,30,10,3, test_data= test_data)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Epoch 0: 7491 / 10000\n","Epoch 1: 7687 / 10000\n","Epoch 2: 8517 / 10000\n","Epoch 3: 9377 / 10000\n","Epoch 4: 9454 / 10000\n","Epoch 5: 9502 / 10000\n","Epoch 6: 9555 / 10000\n","Epoch 7: 9529 / 10000\n","Epoch 8: 9570 / 10000\n","Epoch 9: 9590 / 10000\n","Epoch 10: 9620 / 10000\n","Epoch 11: 9589 / 10000\n","Epoch 12: 9613 / 10000\n","Epoch 13: 9618 / 10000\n","Epoch 14: 9621 / 10000\n","Epoch 15: 9611 / 10000\n","Epoch 16: 9615 / 10000\n","Epoch 17: 9623 / 10000\n","Epoch 18: 9619 / 10000\n","Epoch 19: 9618 / 10000\n","Epoch 20: 9620 / 10000\n","Epoch 21: 9607 / 10000\n","Epoch 22: 9629 / 10000\n","Epoch 23: 9625 / 10000\n","Epoch 24: 9623 / 10000\n","Epoch 25: 9634 / 10000\n","Epoch 26: 9645 / 10000\n","Epoch 27: 9649 / 10000\n","Epoch 28: 9629 / 10000\n","Epoch 29: 9652 / 10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jr-Z7Vv6yrgW","colab_type":"text"},"source":["# Testing Neural Network"]},{"cell_type":"code","metadata":{"id":"UkoP4GXufzhD","colab_type":"code","colab":{}},"source":["predicted = nn.feedforward(training_data[111][0])\n","print(np.argmax(predicted))\n","img = np.reshape(training_data[111][0], (28, 28))\n","plt.imshow(img, cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzgVZwnNzbcM","colab_type":"code","colab":{}},"source":["!pip install albumentations\n","!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6r4V0f60Gdn","colab_type":"code","colab":{}},"source":["!git clone https://github.com/Rodrigo-Burciaga/DataSets-and-Utils-NNDL.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_b1V5Q21ASy","colab_type":"code","colab":{}},"source":["img = cv2.imread('DataSets-and-Utils-NNDL/n_2.jpg', 0)\n","plt.imshow(img, cmap='gray')\n","plt.show()\n","img_rs = cv2.resize(img, (28,28))\n","ret, thresh = cv2.threshold(img_rs, 127, 255, cv2.THRESH_BINARY_INV)\n","plt.imshow(thresh, cmap='gray')\n","plt.show()\n","print(thresh.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"910IampX1bCH","colab_type":"code","colab":{}},"source":["img_to_predict = np.reshape(thresh, (784,1))\n","predicted = nn.feedforward(img_to_predict)\n","img = np.reshape(img_to_predict, (28, 28))\n","title_obj = plt.title('Número predecido: {}'.format(np.argmax(predicted)))\n","plt.setp(title_obj, color='w')  \n","plt.imshow(img, cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]}]}